#+TITLE: Bericht zum Projektpraktikum
#+DESCRIPTION: Für Seminararbeit erstelltes APA7-Dokument mit Blocksatz (abweichend von APA7)
#+EMAIL: a01468373@unet.univie.ac.at
#+LATEX: \begin{center}{}\end{center}
#+LATEX: \begin{center}{}\end{center}
#+LATEX: \begin{center}{}\end{center}
#+LATEX: \begin{center}\bfseries Praktikumsbericht\end{center}
#+LATEX: \begin{center}{}\end{center}
#+LATEX: \begin{center}Dominik Pegler (a01468373)\end{center}
#+LATEX: \begin{center}Machine Learning in der psychologischen Forschung\end{center}
#+LATEX: \begin{center}Zeitraum: 1.3.--30.4.2022\end{center}
#+LATEX: \begin{center}Betreuer: David Steyrl\end{center}
#+LATEX: \begin{center}Fakultät für Psychologie, Universität Wien\end{center}
#+LATEX: \begin{center}13. Mai 2022\end{center}
#+LATEX: \begin{center}{}\end{center}
# +LATEX: \begin{center}Wortanzahl: 5275\end{center}
#+SETUPFILE: setupfile_article_fls.org
#+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/ref.bib}\addtolength\parskip{\medskipamount}

#+LATEX:\newpage
#+LATEX:\tableofcontents
#+LATEX:\newpage
# export with C-c C-e l o

# Tabellen zum Veranschaulichen der Stichprobenzusammensetzungen fehlt noch (für Anhang)!
* Wordcount                                                       :noexport:
#+begin_src python :session *py* :eval yes :exports none :results output
    gesamt = 4000
    abstract = int(.10 * gesamt)
    einleitung = int(.20 * gesamt)
    methoden = int(.20 * gesamt)
    ergebnisse = int(.25 * gesamt)
    diskussion = int(.25 * gesamt)
    print("Ziel Wortanzahl GESAMT: ",gesamt)
    print("abstract:               ",abstract)
    print("einleitung:             ",einleitung)
    print("methoden:               ",methoden)
    print("ergebnisse:             ",ergebnisse)
    print("diskussion:             ",diskussion)
#+end_src
#+RESULTS:
#+RESULTS:
: 5275 words in buffer.
#+begin_src emacs-lisp :eval yes :exports none
(org-wc-display())
(org-wc-put-overlay)))
#+end_src

* Kurzfassung

Im Praktikum wurde ein bestehender Datensatz aus einer psychologischen
Untersuchung mit Methoden aus dem Machine-Learning (ML) analysiert,
wozu eigens ein Script in der Programmiersprache Python angefertigt
wurde. Weitere Applikationen von ML im psychologischen Kontext wurden
diskutiert und Literaturrecherche betrieben.

* Einstieg

Meine Interessen gehen in viele Richtungen, aber in den letzten zwei
bis drei Jahren wurde mein Interesse für die Schnittstellen der
Psychologie mit der Informatik stärker und damit auch der Wunsch
Erfahrungen auf diesem Gebiet zu sammeln. Das hat auch damit zu tun,
dass ich das Psychologiestudium berufsbedingt für eineinhalb Jahre
unterbrechen musste und in dieser Zeit vermehrt mit der
Programmiersprache Python und den Data-Science-Programmpaketen zu tun
hatte. Als ich wieder ins Studium eingstiegen war und die Möglichkeit
eines Pflichtpraktikums im Bereich Machine Learning auf der
Fakultätsseite sah, war für mich der Wunsch da, dieses zu absolvieren.

Meine Erwartungen an das Praktikum waren hoch, was den Wissenserwerb
betraf. Nicht so hoch waren die Erwartungen bezogen auf den
persönlichen Austausch, da hier aufgrund der Pandemie Einschränkungen
zu erwarten waren. David Steyrl lernte ich auch kurz zuvor in einer
Vorlesung kennen, womit ich mir schon ein ungefähres Bild von
seiner Herangehenswese machen konnte.

* Praktikumsstelle

  Der Forschungsbereich von Prof. Frank Scharnowski beschäftigt sich
  vor allem mit Neurofeedback und Computer-Brain-Interfaces. Ein Ziel
  seiner Forschunsgruppe ist es, ein besseres Verständnis zur
  Wirksamkeit von Konfrontationstherapie zu erlangen und diese
  Therapieform für den Einsatz im klinischen Bereich zu
  verbessern. Dabei werden neben psychologischen Merkmalen bildgebende
  Verfahren verwendet, um schlussendlich Closed-Loop-Computermodelle
  der Funktionsweise der Therapie zu erstellen. Weitere Schwerpunkte
  sind Neurofeedback mittels fMRT sowie Machine Learning für
  Datenanalysen. Mein Betreuer David Steyrl arbeitet als PostDoc im
  Team von Frank Scharnowski und ist insbesondere für die Durchführung
  von ML-Datenanalysen und das Erstellen von Computermodellen
  verantwortlich. Seine weiteren Forschungsinteressen umfassen
  Biosignalverarbeitung und simultane EEG-fMRT-Messungen. David Steyrl
  studierte Medizintechnik und Data Science an der TU Graz.

* Ziele

Ziele des Praktikums waren für mich, eine Verbindung zwischen
psychologischer Forschung und modernen Machine-Learning-Methoden
herzustellen. Bisher kannte ich hier kaum konkrete
Anwendungsfälle. Mit dem Betreuer David Steyrl wurde daher vereinbart,
einen bereits mit klassischen statistischen Methoden (Korrelationen,
Moderationsanalyse) untersuchten Datensatz, der im Rahmen meiner
Bachelorarbeit erhoben wurde und Zusammenhänge zwischen
problematischem Alkoholkonsum und Mindfulness (sowie einer Reihe
anderer Prädiktoren), mit ML-Methoden zu untersuchen. Dabei sollten
nicht die Ergebnisse der früheren und der aktuellen Analyse
gegenübergestellt werden, sondern ein Gefühl entwickelt werden, wie
eine solche ML-Analyse in der Praxis funktioniert, da mir bisher die
Anwendungsfälle fehlten. Weiteres Ziel war für mich eine Vertiefung des bisherigen Wissens in Form
zusätzlicher Literaturrecherchen.

* Ablauf

Die Dauer des Praktikums wurde mit mit 8 Wochen zu je 30
Arbeitsstunden pro Woche und aufgrund der Pandemiesituation auf einen
Online-Modus festgelegt. Konkrekt bedeutete das für mich eine
Arbeitswoche von Montag bis Samstag, in der mehrmalige Videocalls per
Zoom mit meinem Betreuer stattfanden. Zusätzlich wurde immer wieder
auch per Email oder Skype kommuniziert. Einmal wöchentlich fand ein
Online-Meeting mit jenen MasterstudentInnen statt, die David Steyrl im
Rahmen deren Masterarbeit betreute. Insgesamt schätze ich, dass sich
die aufgewendete Zeit zu 50% auf Programmiertätigkeiten, etwa zu 10%
auf die Kommunikation und zu weiteren 40% auf Literaturrecherche,
Analyse und Ergebnissereflexion aufteilte.

* Tätigkeit

** Analyse des Datensatzes
Zu allererst ging es für mich darum, Vorkenntnisse aufzufrischen und
die Methode der linearen Regression mit Python zu wiederhohlen, da
diese als der Ausgangspunkt jeden maschinellen Lernens gesehen werden
kann. Im nächsten Schritt wurden regularisierte lineare
Regressionsmodelle und deren Hyperparameter hinzugenommen:
"Ridge"-Regression, "Lasso"-Regression und deren Hybridversion:
"Elastic Net". Danach kamen non-lineare Modelle wie "Random Forests", "Extra
Trees" und "Gradient Boosting" ins Spiel. Aufgrund der Fragestellung mit
einer kontinuierlichen Outcome-Variable wurden keine
Klassifikationsalgorithmen in diesem Praktikum verwendet.
*** Der Datensatz

Der verwendete Datensatz stammte aus meiner eigenen Bachelorarbeit und
wurde im Sommer 2021 über einen Online-Fragebogen erhoben. 500
Versuchspersonen machten darin u.a. Angaben zu ihrem Alkoholkonsum,
Ausprägung von Impulsivität, Mindfulness und Bewältigungsmotive beim
Alkoholkonsum. Zusammen mit den soziodemografischen Variablen wurden
insgesamt 50 Variablen als Prädiktoren in den Analysen verwendet.

*** Erstellung des Scripts

Für die Analyse der Daten war es notwendig, ein Script zu erstellen,
dass die folgenden Aufgaben erfüllt: (1) Einlesen der Daten, (2)
Bereinigung der Daten, indem auf die relevanten Variablen reduziert
wird, diese umbenannt und fehlende Werte imputiert werden, (3)
Standardskalierung der metrischen Prädiktoren, (4) Aufteilung der
Daten in Trainings- und Testsets, (5) äußere Kreuzvalidierung, um
Overfitting zu vermeiden, (6) innere Kreuzvalidierung, um die
optimalen Hyperparameter zu ermitteln, (7) Verfügung über ein Pool an
Modellen (ElasticNet, ExtraTrees und GradientBoostRegressor) und dazu
passende Hyperparameter-Räume, (8) jedes Modell einmal durchlaufen
lassen, (9) Aufzeichnen der R²-Scores, (10) Aufzeichnen der
Shapely-Werte für die Wichtigkeit der einzelnen Prädiktoren.

Es dauerte dabei eine Weile, bis alle Fehler im Script beseitigt waren
und es vernünftige Resulate lieferte. Um auch die letzte Skepsis zu
beseitigen, wurden dieselben Daten zusätzlich mit einem fertigen
Script von David Steyrl analysiert. Beide lieferten nahezu dieselben
Ergebnisse.

*** Ergebnisdarstellung

Bis das Script fehlerfreie Resultate lieferte, wurde es dreimal
komplett durchlaufen, wobei dies jeweils etwa 12 Stunden in Anspruch
nahm. Die Resultate wurden anschließend für jeden Lauf in einem
eigenen Jupyter-Notebook (verfügbar im [[https://github.com/dominikpegler/internship_ml/blob/main/results.ipynb][Online-Repository]])
aufbereitet. Diese enthalten die Outputs des Scripts (R²-Score jedes
Modells pro Kreuzvalidierungs-Split) sowie die Plots zur Wichtigkeit
der einzelnen Prädiktoren (Shapley-Werte). Abbildung
ref:shapley_values_sum illustriert, welche Wichtigkeit die einzelnen
Prädiktoren im jeweiligen Modell (im konkreten Beispiel der
ExtraTreesRegressor aus scikit-learn) auf Basis der Shapley-Werte für
die Variable Alkoholkonsum hatten. Abbildung ref:shapley_values zeigt
ein Beispiel, wie die Vorhersage einer einzelnen Versuchsperson
mittels Shapley-Values erklärt werden kann. Die Höhe der Werte gibt
die Wichtigkeit der einzelnen Prädiktoren für den Outcome
an. Prädiktoren mit roten Balken drücken den Wert des Outcomes nach
oben, bei blauen Balken ist es genau umgekehrt.

#+attr_latex: :width 300
#+name: shapley_values_sum
#+caption: Zusammenfassung der Wichtigkeit der einzelnen Prädiktoren für die Variable Alkoholkonsum mittels "Shapley values".
[[file:img/shap_summary.png]]

#+attr_latex: :width 250
#+name: shapley_values
#+caption: Erklärung der Vorhersage der Variable Alkoholkonsum mittels "Shapley values"
[[file:img/shap_explainer.png]]

*** Online-Material

Zur besseren Nachvollziehbarkeit wurden alle Skripte, Notizen,
Resultate sowie der verwendete Datensatz in einem eigens
eingerichteten Online-Repository verfügbar gemacht
(https://www.github.com/dominikpegler/internship_ml). Außerdem wurde
eine einfache Webseite eingerichtet, die mir als Überblick
über Fortschritt und anstehende Tasks diente
(https://dominikpegler.github.io/internship_ml/).

** Theorie: Künstliche neuronale Netzwerke

Im Zuge des Praktikums versuchte ich auch, die Grundprinzipien davon
zu verstehen, wie künstliche neuronale Netzwerke lernen: Aktivierung
und Aktualisierung der Gewichte (Weights) und Biases. In Abbilung
ref:nn_OR ist die simpelste Form dargestellt: ein Neuron $y$ mit zwei
Eingängen $x_0$ und $x_1$, von deren Aktivierungsmuster (in diesem
Fall die OR-Funktion) es abhängt, ob das Neuron feuert ($y = 1$) oder
nicht ($y = 0$). Das Neuron wählt als Ausgangspunkt beim Lernen des
OR-Zusammenhangs erst zufällige Werte für beide Gewichte $w_0$ und
$w_1$) und den Bias ($b$). Über eine Aktivierungsfunktion (hier die
Sigmoid-Funktion) wird aus den Eingangswerten, den Gewichten und dem
Bias ein Wert berechnet, der entweder 0 oder 1 ergibt. Je nachdem, wie
weit das Neuron mit seiner Berechnung vom tatsächlichen Wert abweicht
("Error"), passt es seine Gewichte und den Bias an. Über mehrere
Durchgänge ("Epochs") hinweg, wird die Schätzung des Neurons somit
immer genauer (Abb. ref:NN_learning_rate). Siehe hierzu die
Gradientenberechnung in der nachfolgenden Implementation mit
Programmpaket numpy:


#+attr_latex: :width 200
#+name: nn_OR
#+caption: "Netzwerk" mit nur einem Neuron $y$ lernt die OR-Funktion
[[file:img/nn_OR.jpg]]

*** Beispiel: Training eines einzelnen Neurons in numpy

#+begin_src python -n :session *py* :eval never-export :exports both :results output
import numpy as np
import matplotlib.pyplot as plt

# create input matrix X
X = np.zeros((4, 2), dtype = float)
X[0, :] = [0., 0.]
X[1, :] = [0., 1.]
X[2, :] = [1., 0.]
X[3, :] = [1., 1.]


# target vector 
Y_or = np.array([0., 1., 1., 1.])

N = X.shape[0] # number of input patterns

# create arrays for weights and bias
w = np.random.randn(2)
b = np.random.randn(1)

alpha = 0.05 # learning rate
n_epochs = 5_000 

def g_logistic(net):
    return 1. / (1. + np.exp(-net))

def loss(yhat, y):
    return (yhat - y)**2

def print_forward(x, yhat, y):
    print(f" input  = {x.astype(int)}")
    print(f" output = {yhat.item():.3f}")
    print(f" target = {y.item():.0g}")

def print_grad(grad_w, grad_b):
    print(f"  w_0 = {grad_w[0]: .3f}")
    print(f"  w_1 = {grad_w[1]: .3f}")
    print(f"  b   = {grad_b[0]: .3f}")

# Crucial part: Computing the gradients   
# gradient is derivative of error (cost) divided by derivative of weights (or bias, respectively)
# x_i is not used, if calculated for bias is replaced by 1
def grad_func(y, yhat, net, g, x_i = 1):
    grad = 2 * (yhat - y) * g(net) * (1 - g(net)) * x_i
    return grad    

track_error = []

for epoch in range(n_epochs):
    error_epoch = 0. # sum loss across the epoch
    perm = np.random.permutation(N)
    
    for p in perm: # visit data points in random order
        x = X[p, :] # input pattern
        
        # compute output of neuron
        net = np.dot(x, w) + b
        yhat = g_logistic(net)
        
        # compute loss
        y = Y_or[p]
        myloss = loss(yhat, y)
        error_epoch += myloss.item()
        
        # print output if this is the last epoch
        if (epoch == n_epochs - 1):
            print("\nFinal result:")
            print_forward(x, yhat, y)
            print("")
       
        w_grad = grad_func(y, yhat, net, g_logistic, x)  
        b_grad = grad_func(y, yhat, net, g_logistic, 1) 
                               
        # parameter update with gradient descent
        w -= alpha * w_grad
        b -= alpha * b_grad
            
    track_error.append(error_epoch)
    if epoch % 500 == 0:
        print(f"epoch {epoch}")
        print(f"  err = {error_epoch: .3f}")
        print_grad(w_grad, b_grad)
        print("")
        
fig, ax = plt.subplots()
ax.plot(track_error)
ax.set_title("stochastic gradient descent (logistic activation)")
ax.set_ylabel("error for epoch")
ax.set_xlabel("epoch")
fig.savefig("img/nn_OR_lr.png", dpi = 300)
#+end_src

#+RESULTS:
#+begin_example
epoch 0
  err =  1.081
  w_0 = -0.000
  w_1 = -0.029
  b   = -0.029

epoch 500
  err =  0.156
  w_0 = -0.001
  w_1 = -0.001
  b   = -0.001

epoch 1000
  err =  0.074
  w_0 =  0.000
  w_1 =  0.000
  b   =  0.068

epoch 1500
  err =  0.046
  w_0 = -0.000
  w_1 = -0.000
  b   = -0.000

epoch 2000
  err =  0.033
  w_0 = -0.000
  w_1 = -0.013
  b   = -0.013

epoch 2500
  err =  0.026
  w_0 = -0.010
  w_1 = -0.000
  b   = -0.010

epoch 3000
  err =  0.021
  w_0 = -0.000
  w_1 = -0.000
  b   = -0.000

epoch 3500
  err =  0.017
  w_0 = -0.000
  w_1 = -0.007
  b   = -0.007

epoch 4000
  err =  0.015
  w_0 = -0.000
  w_1 = -0.000
  b   = -0.000

epoch 4500
  err =  0.013
  w_0 =  0.000
  w_1 =  0.000
  b   =  0.014


Final result:
 input  = [0 1]
 output = 0.950
 target = 1


Final result:
 input  = [1 1]
 output = 1.000
 target = 1


Final result:
 input  = [1 0]
 output = 0.950
 target = 1


Final result:
 input  = [0 0]
 output = 0.081
 target = 0
#+end_example

#+attr_latex: :width 250
#+name: NN_learning_rate
#+caption: Beispiel: Fehlerrate verringert sich mit zunehmender Anzahl an Durchgängen
[[file:img/nn_OR_lr.png]]

* Hilfreiche Literatur

Wichtige Literatur, die ich insbesondere in den ersten zwei Wochen
immer wieder herangezogen habe, waren die offizielle Dokumentation des
Softwarepakets Scikit-Learn
parencite:scikit-learndevelopersScikitlearnUserGuide2022 und die erste
Hälfte von textcite:geronHandsOnMachineLearning2019. Diese behandelt
klassische Machine-Learning-Algorithmen, wie sie in Scikit-Learn
implementiert sind, die zweite Hälfte neuronale Netzwerke mit
Tensorflow.
* Reflexion


** Bezug zu bisherigen Lehrinhalten

Im Verlauf des Praktikums war vor allem mein Vorwissen aus dem
Bachelorstudium zu Statistik und Forschungsmethoden hilfreich.

Insgesamt seien hier folgende Lehrveranstaltungen genannt:

- Fachliteraturseminar und Bachelorarbeit bei Ulrich Tran
- Vorlesung Ausgewählte Methoden aus dem Bachelorstudium von Ulrich Tran
- Vorlesung Testtheorie aus dem Bachelorstudium von Michael Weber
- Vorlesung Differenzielle Psychologie von Georg Gittler
- Vorlesung Statistik für Fortgeschrittene aus dem Masterstudium von Ulrich Tran

Herausstreichen möchte ich hier die Themenblöcke zur multiplen
linearen Regression aus der Vorlesungsreihe "Statistik für
Fortgeschrittene" von Ulrich Tran. Ein großer Teil der Tätigkeiten im
Praktikum verlangt jedoch Wissen aus Bereichen, die wenig behandelt
wurden (vor allem nicht im Bachelorstudium): Machine Learning und
Programmierung. Das Praktikum erfordert daher entweder ein hohes Maß
an Lernbereitschaft oder Vorwissen in diesen Bereichen.

** Programmieren in Python
Aufgrund meiner Vorerfahrungen mit Programmierung, insbesondere mit
Python, fiel es mir nicht schwer, den für die Analyse notwendigen
Programmcode zu schreiben. Die Dokumentationen der Programmpakete sind
frei zugänglich im Internet, sehr detailliert und einfach zu lesen. Es
musste hauptsächlich der Zusammenhang zwischen der Theorie und der
tatsächlichen Implementation in Form von Code hergestellt
werden. Schwierigkeiten bereitete mir hier anfänglich das Konzept der
Kreuzvalidierung, da ich diese in meinem Kopf als eine Schleife
verinnerlicht hatte, im Programmpaket scikit-learn ist dies allerdings
bereits auf eine einzelne Zeile Code reduziert (siehe nachfolgendes
Beispiel). Das nimmt einem zwar viel Arbeit ab, jedoch dauerte es
etwas festzustellen, dass sich hinter dieser einen Zeile tatsächlich
eine Schleife verbirgt (nicht sichtbar für den Anwender, außer man
wirft einen Blick in den Source-Code).
*** Beispiel: Innere Kreuzvalidierungsschleife
#+begin_src python -n :eval never-export
# nested CV loop for parameter optimization
inner_cv = ShuffleSplit(n_splits = 50, test_size = 0.2) 

# creating the regressor instance
# and passing it the previously created loop
reg_with_bayes_search_cv = BayesSearchCV(
    estimator = reg,
    search_spaces = hyper_space,
    n_iter = 200,
    cv=inner_cv,
    n_jobs = -2,
    random_state = 0)
#+end_src

** Betreuung

Praktikumsbetreuer David Steyrl stand übers gesamte Praktikum hinweg
zur Verfügung, wenn ich Fragen oder Schwierigkeiten mit den
Aufgabestellungen hatte. Er hat mir im Erstgespräch und im späteren
Verlauf immer wieder klar gesagt, was er von mir erwartet, und
Feedback zu den von mir verrichteten Tätigkeiten gegeben. Sehr
wertvoll war für mich, dass ich nie das Gefühl hatte, eine für mich
wichtige Frage nicht stellen zu können. In den Gesprächen mit meinem
Betreuer schien es mir auch kein Thema auf dem Gebiet von ML und AI zu
geben, zu dem er keine Erfahrungen hatte. Ich fand das sehr
bereichernd, da es zwar sehr viel Literatur zu ML-Themen gibt, aber
jemanden zu finden, der persönlich aus der Praxis berichten
kann, ist nicht immer so einfach.

Wie eingangs erwähnt, fand einmal wöchentlich ein
Online-Meeting via Skype mit den MasterstudentInnen statt, deren
Masterarbeit David Steyrl zu dem Zeitpunkt betreute. Das Kennenlernen
anderer Studierender, deren Herangehensweisen und Fortschritte war
insgesamt eine sehr interessante, aufschlussreiche und wichtige
Erfahrung für mich im Hinblick auf die eigene bevorstehende
Masterarbeit.

Aufgrund von Krankheit waren insgesamt zwei der acht Wochen leider
weniger produktiv; in dieser Zeit fand deshalb kaum Austausch zwischen
meinem Betreuer und mir statt.

Was im Praktikum vielleicht etwas fehlte, war der direkte Bezug zu
aktuellen Forschungsthemen an der Praktikumsstelle. Das hätte z.B. in
Form von Analysen an bestehenden Datensätzen aus
Neurofeedback-Untersuchungen passieren können. Auf der anderen Seite
hätte ein solches Vorgehen wahrscheinlich zur Folge gehabt, dass man
viel Zeit mit dem Verstehen eines solchen Datensatzes verbracht hätte
und für die tatsächliche Anwendung von ML-Methoden nicht ausreichend
Zeit geblieben wäre. Mit dem mir bereits bekannten Datensatz aus
meiner Bachelorarbeit fiel diese Problematik weg.

** Ausblick

# Was sind die Take-Away-Points? Was habe ich besonders gelernt?

Ich nehme aus dem Praktikum viel neues Wissen und ein besseres
Verständnis für die verschiedenen Methoden und deren Sinn mit. Nennen
könnte man hier die Kreuzvalidierungsschleifen, etwas, das
ich aus bisheriger Literatur nicht in der Form kannte. Auch für die
verschiedenen Optimierungsmethoden zum Finden passender Hyperparameter
habe ich nun ein tieferes Verständnis entwickelt. Was mir David Steyrl
ebenfalls mitgegegeben hat, ist, dass man sich bei all dem aktuellen (zum
Teil auch berechtigten) Optimismus und Hype rund um das Thema Machine
Learning eine gute Portion Realismus beibehalten und sich vor
übertriebenen Erwartungen in Acht nehmen sollte.

Es sind viele Dinge, auf denen ich jetzt aufbauen kann. Das im
Praktikum erstellte Skript kann ich beispielsweise in Zukunft für
weitere Anwendungen verwenden. Weiters könnte ich mich auch den Themen
Bilderklassifikation mittels künstlicher neuronaler Netzwerke
widmen. Dieses Thema wurde im Praktikum zwar nicht ausführlich
behandelt, jedoch habe ich hier ebenfalls einige praktische
Erfahrungen gesammelt und Informationen von meinem Betreuer
erhalten. Die praktische Relevanz solcher Methoden in der
psychologischen Forschung gilt es dabei noch zu ermitteln. Für mich
haben sich durch das Praktikum nun einige gute Ausgangspunkte ergeben,
nicht zuletzt auch für die eigene Masterarbeit, die ich im nächsten
Semester beginne.
* Textstücke                                                       :noexport:
** Einleitung

Mein Pflichtpraktikum für das Masterstudium Psychologie habe ich
absolviert direkt an der Universität Wien, und zwar am Institut für
Emotion, Kognition und Methoden. Genauer sagt bei David Steyrl, der im
Lab von Frank Scharnowski arbeitet, und sich dort hauptsächlich mit
Machine Learning im psychologischen Forschnugskontext beschäftigt.

Motivation: Warum diese Stelle?



** Tätigkeit

# vielleicht auch was zu den Zielen des Praktikums anführen (das am
# Ende vergleichen).

Am ersten Tag des Praktikums ist es mal um die Ausrichtung gegangen,
welche Themen würden mich interessieren, welche Tätigkeiten sind
möglich. Beim Einführungsgespräch haben wir uns darauf geeinigt, dass
ich mir den Datensatz aus der Bachelorarbeit hernehmen
(Sommersemester 2021) und diesen diesmal mit ML-Methoden bearbeite und
sehe, ob die Ergebnisse ähnlich aussehen wie damals mit klassischen
statistischen Methoden. Zur Erinnerung: Der Datensatz umfasste
Variablen wie Bewältigungsmotive für Alkoholkonsum, Impulsivität und
Mindfulness und die Bachelorarbeit kam zum Ergebnis, dass diese
Variablen mit problematischem Alkoholkonsum zusammenhingen
(Korrelationen). Im Unterschied dazu war jetzt vorgesehen, 1) alle
diese Variablen in Form ihrer einzelnen Fragebogen-Items in ein
einziges Machine-Learning-Modell zu packen und zu sehen, wie gut
dieser Verbund an Variablen in der Lage ist, Alkoholkonsum
vorherzusagen. 2) Jene Items (Features) zu identifizieren, die die
stärkste Vorhersagekraft besitzen.

Wie bin ichs angegangen?


Was hab ich gemacht?


Welchen Code hab ich geschrieben? Wo liegt der Code, wie ist er organisiert?



* Literatur
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
#+LaTeX: \printbibliography[heading=none]

#+latex: \clearpage
* Appendix                                                         :noexport:
  :PROPERTIES:
  :UNNUMBERED: t
  :END:

#+latex: \FloatBarrier

Hier könnte some beispielcode landen.

zB.

#+begin_src python -n :session *py* :eval never-export :exports both :results output

class BeispielCode():

    def __init__(self):
        pass
    def __call__(self):
        return "this is the example output!"

example = BeispielCode()

print(example())

#+end_src

#+RESULTS:
: this is the example code

* noexport                                                         :noexport:
** Biblibography link                                             :noexport:

Notwendig, um die Zitate im Buffer als Links darzustellen (=> zu helm-bibtex)

bibliography:ref/ref.bib
** Local functions                                        :noexport:
  :PROPERTIES:
  :EXPORT_TITLE: Annotated Bibliography Template
  :EXPORT_OPTIONS: tags:nil todo:nil
  :END:
*** User Entities
The following source code block sets up user entities that are used frequently
in my work. I use the various =.*macron= commands to typeset Hawaiian
language words with what is known in Hawaiian as a /kahak\omacron{}/.

The =space= entity is useful following a period that doesn't end a
sentence. LaTeX sets a space slightly longer than an inter-word space
following a sentence ending period. The =space= entity lets LaTeX know
to set an inter-word space.

#+name: user-entities
#+begin_src emacs-lisp
  (setq org-entities-user nil)
  (add-to-list 'org-entities-user '("space" "\\ " nil " " " " " " "–"))
  (add-to-list 'org-entities-user '("amacron" "\\={a}" nil "&#0257" "a" "a" "ā"))
  (add-to-list 'org-entities-user '("emacron" "\\={e}" nil "&#0275" "e" "e" "ē"))
  (add-to-list 'org-entities-user '("imacron" "\\={\\i}" nil "&#0299" "i" "i" "ī"))
  (add-to-list 'org-entities-user '("omacron" "\\={o}" nil "&#0333" "o" "o" "ō"))
  (add-to-list 'org-entities-user '("umacron" "\\={u}" nil "&#0363" "u" "u" "ū"))
  (add-to-list 'org-entities-user '("Amacron" "\\={A}" nil "&#0256" "A" "A" "Ā"))
  (add-to-list 'org-entities-user '("Emacron" "\\={E}" nil "&#0274" "E" "E" "Ē"))
  (add-to-list 'org-entities-user '("Imacron" "\\={I}" nil "&#0298" "I" "I" "Ī"))
  (add-to-list 'org-entities-user '("Omacron" "\\={O}" nil "&#0332" "O" "O" "Ō"))
  (add-to-list 'org-entities-user '("Umacron" "\\={U}" nil "&#0362" "U" "U" "Ū"))
#+end_src
*** LaTeX Process
The Org mode variable =org-latex-pdf-process= holds a list of strings,
each of which is run as a shell command. Typically, several commands
are needed to process a LaTeX document to produce pdf output. The
following two source code blocks use a straightforward approach that
should work in most cases. The source code block named
[[set-pdf-process-bibtex][set-pdf-process-bibtex]] uses [[http://www.bibtex.org/Using/][BibTeX]] to process the bibliography. BibTeX
has been a standard for many years in the LaTeX world. The source code
block named [[set-pdf-process-biber][set-pdf-process-biber]] uses a newer bibliography processor
named [[http://biblatex-biber.sourceforge.net/][Biber]], which is designed to work with [[http://www.ctan.org/pkg/biblatex][BibLaTeX]].  The choice of
which one to use must be reflected in the =usepackage= command for
BibLaTeX at the top of this file; the optional command =backend= takes
either =bibtex= or =biber= as its value.

At a practical level, perhaps the main difference between Biber and
BibTeX is how they handle special characters. The bibliographic
database for BibTeX uses LaTeX commands for special characters while
the database for Biber uses UTF-8 characters.

#+name: set-pdf-process-bibtex
#+header: :results silent
#+begin_src emacs-lisp
  (setq org-latex-pdf-process
        '("pdflatex -interaction nonstopmode --shell-escape -output-directory %o %f"
          "bibtex %b"
          "pdflatex -interaction nonstopmode --shell-escape -output-directory %o %f"
          "pdflatex -interaction nonstopmode --shell-escape -output-directory %o %f"))
#+end_src

#+name: set-pdf-process-biber
#+header: :results silent
#+begin_src emacs-lisp
  (setq org-latex-pdf-process
        '("pdflatex -interaction nonstopmode --shell-escape -output-directory %o %f"
          "biber %b"
          "pdflatex -interaction nonstopmode --shell-escape -output-directory %o %f"
          "pdflatex -interaction nonstopmode --shell-escape -output-directory %o %f"))
#+end_src


*** Cite Link
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-06 Sat 21:11
:ARCHIVE_FILE: ~/Dropbox/org/mindfulness_lit.org
:ARCHIVE_OLPATH: TODOS
:ARCHIVE_CATEGORY: fls-article
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: noexport
:END:
There are many ways to manage citations in Org mode. My preference is
to manage the bibliography database with [[http://joostkremers.github.io/ebib/][Ebib: a BibTeX database
manager for Emacs]] and insert citations using a custom Org mode link. I
find the work flow convenient and the look of the Org mode buffer
"good enough."

The source code block named [[ebib-setup][ebib-setup]] defines a cite command that
[[http://joostkremers.github.io/ebib/][Ebib]] will use to insert citations in an Org mode buffer. It inserts
the BibTeX key as the path part of the link and then offers the user
three prompts to enter strings separated by semi-colons as the
description part of the link. The first of these typically holds a
page number, the second holds a string that appears before the in-text
citation (typically, something like "e.g.,"), and the third is the
description of the citation visible in the Org mode buffer.

The source code block named [[define-biblatex-cite-link][define-biblatex-cite-link]] defines an Org
mode link type that parses the link inserted by [[http://joostkremers.github.io/ebib/][Ebib]] and outputs a
correctly formatted LaTeX citation. In theory, it is possible also to
export correctly formatted citations to other backends, but the link
type defined here doesn't do that. The html export simply sandwiches
the BibTeX key between =<cite>= tags and is included here as a
placeholder for future development.

#+name: ebib-setup
#+begin_src emacs-lisp
  (setq ebib-citation-commands
        (quote ((any (("cite" "\\cite%<[%A]%>{%K}")))
                (org-mode (("cite" "[[cite:%K][%A;%A;%A]]"))))))
#+end_src

#+name: define-biblatex-cite-link
#+begin_src emacs-lisp :results silent
  (org-add-link-type 
   "cite" 'ebib
   (lambda (path desc format)
     (cond
      ((eq format 'html)
       (format "(<cite>%s</cite>)" path))
      ((eq format 'latex)
       (if (or (not desc) (equal 0 (search "cite:" desc)))
           (format "\\cite{%s}" path)
         (format "\\cite[%s][%s]{%s}"
                 (cadr (split-string desc ";"))
                 (car (split-string desc ";"))  path))))))
#+end_src

*** Koma Article
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-06 Sat 21:10
:ARCHIVE_FILE: ~/Dropbox/org/mindfulness_lit.org
:ARCHIVE_OLPATH: TODOS
:ARCHIVE_CATEGORY: fls-article
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: noexport
:END:
The following two source code blocks set up a LaTeX class named
=koma-article= that is referenced near the top of the file. The
=koma-article= class is based on the [[http://www.ctan.org/pkg/koma-script][Koma script]] article class
=scrartcl=, which uses a sans-serif font for headings and a serif font
for body text.

The =koma-article= class uses fonts from the [[http://www.gust.org.pl/projects/e-foundry/tex-gyre/][TeX Gyre collection of
fonts]]. As explained in [[http://www.gust.org.pl/projects/e-foundry/tex-gyre/tb87hagen-gyre.pdf][The New Font Project: TeX Gyre]], a goal of the
project was to produce good quality fonts with diacritical characters
sufficient to cover all European languages as well as Vietnamese and
Navajo. 

The source code block named [[koma-article-times][koma-article-times]] is based on the Times
Roman font. The serif Termes font is a replacement for Times Roman,
the sans-serif Heros font is a replacement for Helvetica, and the
typewriter Cursor font is a replacement for Courier. The source code
block named [[koma-article-palatino][koma-article-palatino]] is based on the beautiful Palatino
font designed by Hermann Zapf. The Pagella font is the TeX Gyre
replacement for Palatino. Typographers often recommend that
linespacing be increased slightly with Palatino, and this has been
achieved with the addition of the =linespacing= command.

The Tex Gyre fonts benefit from the [[http://ctan.org/tex-archive/macros/latex/contrib/microtype][microtype package]], which provides
"subliminal refinements towards typographical perfection," including
"character protrusion and font expansion, furthermore the adjustment
of inter-word spacing and additional kerning, as well as hyphenatable
letter spacing (tracking) and the possibility to disable all or
selected ligatures."

In addition, the [[http://www.ctan.org/tex-archive/macros/latex/contrib/paralist/][paralist package]] is used for its compact versions of
the LaTeX list environments.

Finally, the =newcommand= is provided merely as an illustration of one
way to move LaTeX declarations out of the Org file header. This one is
useful in my work as an archaeologist and over the years it has crept
into my BibTeX database. It shouldn't interfere with your work, but
you might want to remove it or replace it with LaTeX commands that you
do frequently use.

#+name: koma-article-times
#+header: :results silent
#+begin_src emacs-lisp
   (require 'ox-latex)
   (add-to-list 'org-latex-classes
                '("koma-article"
                  "\\documentclass{scrartcl}
                   \\usepackage{microtype}
                   \\usepackage{tgtermes}
                   \\usepackage[scale=.9]{tgheros}
                   \\usepackage{tgcursor}
                   \\usepackage{paralist}
                   \\newcommand{\\rc}{$^{14}C$}"
                  ("\\section{%s}" . "\\section*{%s}")
                  ("\\subsection{%s}" . "\\subsection*{%s}")
                  ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
                  ("\\paragraph{%s}" . "\\paragraph*{%s}")
                  ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src

#+name: koma-article-palatino
#+header: :results silent
#+begin_src emacs-lisp
   (require 'ox-latex)
   (add-to-list 'org-latex-classes
                '("koma-article"
                  "\\documentclass{scrartcl}
                   \\usepackage{microtype}
                   \\usepackage{tgpagella}
                   \\usepackage[scale=.9]{tgheros}
                   \\usepackage{tgcursor}
                   \\usepackage{paralist}
                   \\newcommand{\\rc}{$^{14}C$}"
                  ("\\section{%s}" . "\\section*{%s}")
                  ("\\subsection{%s}" . "\\subsection*{%s}")
                  ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
                  ("\\paragraph{%s}" . "\\paragraph*{%s}")
                  ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src

*** Bibliography link                                            :noexport:
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-06 Sat 21:10
:ARCHIVE_FILE: ~/Dropbox/org/mindfulness_lit.org
:ARCHIVE_OLPATH: TODOS
:ARCHIVE_CATEGORY: fls-article
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: noexport
:END:

Notwendig, um die Zitate im Buffer als Links darzustellen (=> zu helm-bibtex)

bibliography:ref/ref.bib

*** Local variables                                              :noexport:
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-06 Sat 21:10
:ARCHIVE_FILE: ~/Dropbox/org/mindfulness_lit.org
:ARCHIVE_OLPATH: TODOS
:ARCHIVE_CATEGORY: fls-article
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: noexport
:END:

befinden sich ganz am Ende des Dokuments (Fußnoten)


# Local Variables: 
# eval: (and (fboundp 'org-sbe) (not (fboundp 'sbe)) (fset 'sbe 'org-sbe))
# eval: (sbe "koma-article-palatino")
# eval: (sbe "user-entities")
# eval: (sbe "set-pdf-process-biber")
# eval: (sbe "ebib-setup")
# eval: (sbe "define-biblatex-cite-link")
# org-latex-inputenc-alist: (("utf8" . "utf8x"))
# eval: (setq org-latex-default-packages-alist (cons '("mathletters" "ucs" nil) org-latex-default-packages-alist))
# End:


